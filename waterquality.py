# -*- coding: utf-8 -*-
"""WaterQuality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11qF84Zyv8vpf6NWX2kSJymkBp99bSSAd

Importing the Libraries
"""

import os,sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from warnings import filterwarnings
filterwarnings('ignore')


pd.set_option('display.max_columns', None)

data = pd.read_csv('/content/drive/MyDrive/Datasets/water_potability.csv')

"""Reading the dataset"""

data.head()

rows, cols = data.shape[0], data.shape[1]

print("Number of rows are {}".format(rows))
print("Number of columns are {}".format(cols))

data.info()

data.columns

data.isnull().sum()

data.isna().sum()

"""Statistical analysis"""

data.describe()

data.duplicated().sum()

ax = data['Potability'].value_counts().plot(kind='bar', figsize=(6,4))
for i in ax.containers:
  ax.bar_label(i)

for i in data.columns:
  total=[]
  cols= []
  if (data[i].isnull().sum() != 0):
    total.append(data[i].isnull().sum())
    cols.append(i)
total

list1=data.columns[data.apply(lambda col:any(col.isnull()))]

null_proportion = pd.DataFrame()
null_proportion['Features'] = list1

null_proportion['Proportion'] = [data[i].isnull().sum() for i in list1]
null_proportion['% of Null values'] = [i*100/data.shape[0] for i in null_proportion['Proportion']]
null_proportion

for i in list1:
  sns.boxplot(data[i], orient='h')
  plt.show()

"""Missing values Imputation

"""

data['ph'].fillna(data['ph'].median(),inplace=True)
data['Sulfate'].fillna(data['Sulfate'].median(),inplace=True)
data['Trihalomethanes'].fillna(data['Trihalomethanes'].median(),inplace=True)

data.isnull().sum()

"""Detecting Outliers"""

def outliers(data,col):
  mean= data[col].mean()
  std = data[col].std()
  outliers=[]
  for i in data[col]:
    z=(i-mean)/std
    if z>3 or z<-3:
      outliers.append(i)
  print('There are {} outliers in {}'.format(len(outliers), col))
  print('Outliers are {}'.format(outliers))

outliers(data,'Sulfate')

outliers(data,'Solids')

outliers(data,'Chloramines')

"""Uni-variate analysis"""

fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18,16))
sns.violinplot(data['ph'], ax=axes[0,0])
sns.violinplot(data['Hardness'], ax=axes[0,1])
sns.violinplot(data['Solids'], ax=axes[0,2])
sns.violinplot(data['Chloramines'], ax=axes[1,0])
sns.violinplot(data['Sulfate'], ax=axes[1,1])
sns.violinplot(data['Conductivity'], ax=axes[1,2])
sns.violinplot(data['Organic_carbon'], ax=axes[2,0])
sns.violinplot(data['Trihalomethanes'], ax=axes[2,1])
sns.violinplot(data['Turbidity'], ax=axes[2,2])
plt.show()

"""Bi-variate Analysis"""

sns.pairplot(data, hue='Potability')
plt.show()

plt.subplots(figsize=(16,16))
sns.heatmap(data.corr(), annot= True)
plt.show()

"""From the correlation plot we see there is not much correlation between the features."""

X = data.iloc[:,:-1]
y= data.iloc[:,-1]

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=42)

from sklearn.preprocessing import MinMaxScaler

mm = MinMaxScaler()

x_train = mm.fit_transform(x_train)
x_test = mm.transform(x_test)

"""### Model Building

Aritifical Neural Network (ANN)
"""

import tensorflow as tf
from tensorflow import keras
from keras.layers import Dense, Dropout
from keras.models import Sequential

classifier = Sequential()

classifier.add(Dense(units=6,input_dim=9, activation='relu', kernel_initializer='he_uniform'))
classifier.add(Dense(units=16, activation='relu', kernel_initializer='he_normal'))
classifier.add(Dropout(0.25))
classifier.add(Dense(units=8, activation='relu', kernel_initializer='he_uniform'))
classifier.add(Dense(units=1, activation='sigmoid'))

from keras.optimizers import Adam

classifier.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])

model = classifier.fit(x_train,y_train, epochs=100, validation_split=0.2, validation_data=(x_test,y_test))

print(model.history.keys())

plt.plot(model.history['accuracy'], label='Training accuracy', color='green')
plt.plot(model.history['val_accuracy'], label='Validation accuracy', color='blue')
plt.title('Accuracy vs Val accuracy')
plt.legend()
plt.xlabel('No.of epochs')
plt.ylabel('Accuracy')
plt.show()

plt.plot(model.history['loss'], label='Training loss', color='green')
plt.plot(model.history['val_loss'], label='Validation loss', color='blue')
plt.legend()
plt.title('Training loss vs validation loss')
plt.xlabel('No. of epochs')
plt.ylabel('Loss')
plt.show()

pred = classifier.predict(x_test)

pred = np.where(pred>0.5,1,0).flatten()

from sklearn.metrics import accuracy_score, classification_report

print(classification_report(y_test,pred))

"""Hyperparameter Tuning using Keras-tuner"""

pip install keras-tuner

import kerastuner
from kerastuner.tuners import RandomSearch
from keras.losses import hinge
from tensorflow.keras import layers
from tensorflow import keras

def build_model(hp):
  model = keras.Sequential()
  num_hidden_layers = hp.Int('num_hidden_layers', min_value=2,max_value=12,step=1)
  for i in range(num_hidden_layers):
    units= hp.Int('units_'+str(i), 32, 512, step=32)
    activation=hp.Choice(f'activation_layer_{i}',values =['relu', 'sigmoid', 'tanh'])
    model.add(keras.layers.Dense(units=units, activation=activation))
  model.add(keras.layers.Dense(units=1, activation='sigmoid'))
  learning_rate = hp.Choice('learning_rate', values=[0.01,0.001,0.0001])
  model.compile(optimizer= keras.optimizers.Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])

  return model

tuner1 = RandomSearch(build_model, objective='accuracy', max_trials=3, executions_per_trial=2, directory='tuning1', project_name='my_project1')

tuner1.search(x_train,y_train, epochs=5, validation_data=(x_test,y_test))

tuner1.results_summary()

# Model with parameters obtained from Tuning

classifier_1 = Sequential()
classifier_1.add(Dense(units=8, activation='relu', input_dim=9))
classifier_1.add(Dense(units=224, activation='relu'))
classifier_1.add(Dense(units=352, activation='sigmoid'))
classifier_1.add(Dense(units=416, activation='tanh'))
classifier_1.add(Dense(units=480, activation='tanh'))
classifier_1.add(Dense(units=448, activation='sigmoid'))
classifier_1.add(Dense(units=480, activation='sigmoid'))
classifier_1.add(Dense(units=128, activation='sigmoid'))
classifier_1.add(Dense(units=96, activation='sigmoid'))
classifier_1.add(Dense(units=1, activation='sigmoid'))

optimizer = keras.optimizers.Adam(learning_rate=0.001)

classifier_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

x_train.shape, y_train.shape,x_test.shape, y_test.shape

model_1 = classifier_1.fit(x_train,y_train, epochs=50, validation_data=(x_test,y_test), validation_split=0.2)

"""We see ANN model is not performing very good even after fine tuning. However, ANN model performed comparitively better without hyperparameter tuning. Here, size of the data is also very small which might be the reason why we are getting an average performance. We will explore Machine learning algorithms as the size of the data is small and analyse its performance.

Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

lg= LogisticRegression(random_state=32)

lg.fit(x_train,y_train)

lg_pred = lg.predict(x_test)

print(classification_report(y_test,lg_pred))

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

dt5 = DecisionTreeClassifier(max_depth=5, random_state=32)

dt5.fit(x_train,y_train)

dt_pred=dt5.predict(x_test)

print(classification_report(y_test,dt_pred))

